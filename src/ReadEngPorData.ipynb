{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "from pickle import dump\n",
    "from unicodedata import normalize\n",
    "from numpy import array\n",
    "from pickle import load\n",
    "from numpy.random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(lang_key = 'EN'):\n",
    "    db_writer = mysql.connector.connect(\n",
    "        host=\"35.232.80.174\",\n",
    "        database=\"masters\",\n",
    "        user=\"root\",\n",
    "        passwd=\"MBBmasters!\")\n",
    "\n",
    "    query = \"SELECT a.text as EN, b.text as PT FROM masters.Translations left join masters.Sentences a on a.sentence_id=Translations.sentence_id_1 left join masters.Sentences b on b.sentence_id=Translations.sentence_id_2 where Translations.sentence_id_1 in (select sentence_id from masters.Sentences where language_key='EN') AND trim(b.language_key)='PT'\"     \n",
    "    cursor = db_writer.cursor()\n",
    "    cursor.execute(query)\n",
    "\n",
    "    sql_text_data = pd.DataFrame(cursor.fetchall())\n",
    "    sql_text_data.columns = cursor.column_names\n",
    "\n",
    "#     data = cursor.fetchall()\n",
    "#     data = list(data)\n",
    "    # Close the session\n",
    "    db_writer.close()\n",
    "\n",
    "    # Show the data\n",
    "    print(sql_text_data.head())\n",
    "    return sql_text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        EN                          PT\n",
      "0  I have lost my passport     Eu perdi meu passaporte\n",
      "1   Someone stole my money  Algu√©m roubou meu dinheiro\n",
      "2                     Help                     Socorro\n",
      "3      May I have the bill         Pode trazer a conta\n",
      "4     I would like dessert    Eu gostaria de sobremesa\n"
     ]
    }
   ],
   "source": [
    "sentences_df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations(text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.strip().lower()\n",
    "        text = text.replace(punctuation, '')\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean a list of lines\n",
    "def clean_pairs(lines):\n",
    "    cleaned = list()\n",
    "    # prepare regex for char filtering\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    # prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for pair in lines:\n",
    "        clean_pair = list()\n",
    "        for line in pair:\n",
    "            # normalize unicode characters\n",
    "            line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "            line = line.decode('UTF-8')\n",
    "            # tokenize on white space\n",
    "            line = line.split()\n",
    "            # convert to lowercase\n",
    "            line = [word.lower() for word in line]\n",
    "            # remove punctuation from each token\n",
    "            line = [word.translate(table) for word in line]\n",
    "            # remove non-printable chars form each token\n",
    "            line = [re_print.sub('', w) for w in line]\n",
    "            # remove tokens with numbers in them\n",
    "            line = [word for word in line if word.isalpha()]\n",
    "            # store as string\n",
    "            clean_pair.append(' '.join(line))\n",
    "        cleaned.append(clean_pair)\n",
    "    return array(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a list of clean sentences to file\n",
    "def save_clean_data(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EN</th>\n",
       "      <th>PT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i have lost my passport</td>\n",
       "      <td>eu perdi meu passaporte</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>someone stole my money</td>\n",
       "      <td>alguem roubou meu dinheiro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>help</td>\n",
       "      <td>socorro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>may i have the bill</td>\n",
       "      <td>pode trazer a conta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i would like dessert</td>\n",
       "      <td>eu gostaria de sobremesa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        EN                          PT\n",
       "0  i have lost my passport     eu perdi meu passaporte\n",
       "1   someone stole my money  alguem roubou meu dinheiro\n",
       "2                     help                     socorro\n",
       "3      may i have the bill         pode trazer a conta\n",
       "4     i would like dessert    eu gostaria de sobremesa"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test data\n",
    "arr = clean_pairs(array(sentences_df))\n",
    "df = pd.DataFrame(arr)\n",
    "df = df.rename(index=str, columns={0:'EN',1:\"PT\"})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../data/en_pt_test.pkl\n"
     ]
    }
   ],
   "source": [
    "save_clean_data(df, \"../data/en_pt_test.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training data (from por.txt) (Data downloaded from http://www.manythings.org/anki/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a loaded document into sentences\n",
    "def to_pairs(doc):\n",
    "    lines = doc.strip().split('\\n')\n",
    "    pairs = [line.split('\\t') for line in  lines]\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load doc into memory\n",
    "def load_doc(filename):\n",
    "    file = open(filename, mode='rt', encoding='utf8')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../data/english-portuguese_training.pkl\n",
      "[go] => [vai]\n",
      "[go] => [va]\n",
      "[hi] => [oi]\n",
      "[run] => [corre]\n",
      "[run] => [corra]\n",
      "[run] => [corram]\n",
      "[run] => [corre]\n",
      "[run] => [corra]\n",
      "[run] => [corram]\n",
      "[who] => [quem]\n",
      "[wow] => [uau]\n",
      "[wow] => [nossa]\n",
      "[wow] => [wow]\n",
      "[fire] => [fogo]\n",
      "[help] => [ajuda]\n",
      "[help] => [socorro]\n",
      "[jump] => [pule]\n",
      "[jump] => [pulem]\n",
      "[jump] => [pule]\n",
      "[stop] => [pare]\n",
      "[stop] => [parem]\n",
      "[wait] => [espere]\n",
      "[wait] => [espere]\n",
      "[wait] => [esperem]\n",
      "[go on] => [va]\n",
      "[hello] => [oi]\n",
      "[hello] => [alo]\n",
      "[hello] => [ola]\n",
      "[i ran] => [eu corri]\n",
      "[i see] => [estou vendo]\n",
      "[i try] => [eu tento]\n",
      "[i try] => [tento]\n",
      "[i won] => [ganhei]\n",
      "[i won] => [eu venci]\n",
      "[oh no] => [ah nao]\n",
      "[relax] => [relaxe]\n",
      "[relax] => [relaxa]\n",
      "[smile] => [sorria]\n",
      "[smile] => [sorriam]\n",
      "[attack] => [atacar]\n",
      "[attack] => [ataquem]\n",
      "[attack] => [ataque]\n",
      "[cheers] => [saude]\n",
      "[get up] => [levantese]\n",
      "[get up] => [levantemse]\n",
      "[get up] => [levantate]\n",
      "[get up] => [levantese]\n",
      "[get up] => [levantate]\n",
      "[go now] => [va agora]\n",
      "[got it] => [entendi]\n",
      "[got it] => [eu entendi]\n",
      "[got it] => [saquei]\n",
      "[got it] => [entendeu]\n",
      "[he ran] => [ele correu]\n",
      "[he ran] => [ele corria]\n",
      "[hug me] => [me abrace]\n",
      "[i fell] => [eu cai]\n",
      "[i know] => [eu sei]\n",
      "[i know] => [sei]\n",
      "[i left] => [eu sai]\n",
      "[i paid] => [eu paguei]\n",
      "[i quit] => [eu me demito]\n",
      "[i work] => [eu estou trabalhando]\n",
      "[im ok] => [estou bem]\n",
      "[im up] => [estou acordado]\n",
      "[listen] => [escute]\n",
      "[listen] => [oucame]\n",
      "[listen] => [escuta]\n",
      "[listen] => [escutem]\n",
      "[listen] => [ouca isso]\n",
      "[listen] => [escutemme]\n",
      "[listen] => [escute]\n",
      "[listen] => [escuta]\n",
      "[listen] => [escutem]\n",
      "[listen] => [escutai]\n",
      "[no way] => [de jeito nenhum]\n",
      "[no way] => [impossivel]\n",
      "[no way] => [de maneira alguma]\n",
      "[no way] => [de modo algum]\n",
      "[no way] => [sem chance]\n",
      "[really] => [serio]\n",
      "[really] => [e mesmo]\n",
      "[really] => [mesmo]\n",
      "[really] => [e serio]\n",
      "[thanks] => [obrigado]\n",
      "[thanks] => [obrigada]\n",
      "[thanks] => [obrigado]\n",
      "[try it] => [tentao]\n",
      "[try it] => [proveo]\n",
      "[try it] => [provea]\n",
      "[we try] => [tentamos]\n",
      "[we try] => [nos tentamos]\n",
      "[we won] => [vencemos]\n",
      "[we won] => [nos vencemos]\n",
      "[why me] => [por que eu]\n",
      "[ask tom] => [pergunte a tom]\n",
      "[ask tom] => [pergunta para o tom]\n",
      "[ask tom] => [peca para o tom]\n",
      "[ask tom] => [peca ao tom]\n",
      "[ask tom] => [perguntem ao tom]\n"
     ]
    }
   ],
   "source": [
    "# load training dataset\n",
    "filename = '../Test/por-eng/mini_por.txt'\n",
    "\n",
    "doc = load_doc(filename)\n",
    "# split into english-german pairs\n",
    "pairs = to_pairs(doc)\n",
    "# clean sentences\n",
    "clean_data = clean_pairs(pairs)\n",
    "# save clean pairs to file\n",
    "save_clean_data(clean_data, '../data/english-portuguese_training.pkl')\n",
    "# spot check\n",
    "for i in range(100):\n",
    "    print('[%s] => [%s]' % (clean_data[i,0], clean_data[i,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5001, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../data/english-portuguese-training-both.pkl\n",
      "Saved: ../data/english-portuguese-training.pkl\n",
      "Saved: ../data/english-portuguese-validation.pkl\n"
     ]
    }
   ],
   "source": [
    "# reduce dataset size\n",
    "#n_sentences = 10000\n",
    "#ds = clean_data[:n_sentences, :]\n",
    "# random shuffle\n",
    "shuffle(clean_data)\n",
    "# split into train/test\n",
    "train, test = clean_data[:100000], clean_data[100000:]\n",
    "# save\n",
    "save_clean_data(clean_data, '../data/english-portuguese-training-both.pkl')\n",
    "save_clean_data(train, '../data/english-portuguese-training.pkl')\n",
    "save_clean_data(test, '../data/english-portuguese-validation.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5001, 2) (0, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape, test.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
