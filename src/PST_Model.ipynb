{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "import pickle\n",
    "from pickle import dump\n",
    "import re\n",
    "import string\n",
    "from unicodedata import normalize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(row):\n",
    "    # prepare regex for char filtering\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    # prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    # normalize unicode characters\n",
    "    row['text'] = normalize('NFD', row['text']).encode('ascii', 'ignore')\n",
    "    row['text'] = row['text'].decode('UTF-8')\n",
    "    # tokenize on white space\n",
    "    row['text'] = row['text'].split()\n",
    "    # convert to lowercase\n",
    "    row['text'] = [word.lower() for word in row['text']]\n",
    "    # remove punctuation from each token\n",
    "    row['text'] = [word.translate(table) for word in row['text']]\n",
    "    # remove non-printable chars form each token\n",
    "    row['text'] = [re_print.sub('', w) for w in row['text']]\n",
    "    # remove tokens with numbers in them\n",
    "    row['text'] = [word for word in row['text'] if word.isalpha()]\n",
    "    row['text'] = [x.strip(' ') for x in row['text']]\n",
    "    return row['text']\n",
    "\n",
    "\n",
    "#row['text'].strip().lower().replace('(', '').replace(')', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  text  sentence_id\n",
      "0       मैंने अपना पासपोर्ट खो दिया है            3\n",
      "1  mainne apana paasaport kho diya hai            4\n",
      "2          किसी ने मेरा पैसा चुरा लिया            7\n",
      "3       kisee ne mera paisa chura liya            8\n",
      "4                                  मदद           11\n",
      "                                         text  sentence_id\n",
      "0                                          []            3\n",
      "1  [mainne, apana, paasaport, kho, diya, hai]            4\n",
      "2                                          []            7\n",
      "3       [kisee, ne, mera, paisa, chura, liya]            8\n",
      "4                                          []           11\n",
      "                                         text  sentence_id\n",
      "1  [mainne, apana, paasaport, kho, diya, hai]            4\n",
      "3       [kisee, ne, mera, paisa, chura, liya]            8\n",
      "5                                     [madad]           12\n",
      "7         [kya, mujhe, bil, mil, sakata, hai]           16\n",
      "9                 [main, mithaee, chaahoonga]           20\n"
     ]
    }
   ],
   "source": [
    "# import and clean all language pkl files\n",
    "#EN = English, PT = Portuguese, HI = Hindi\n",
    "pkl_file = open('../data/sentences_EN.pkl', 'rb')\n",
    "sentences = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "sentences['text']=sentences.apply(clean_string, axis=1)\n",
    "clean_EN_df = sentences\n",
    "\n",
    "pkl_file = open('../data/sentences_PT.pkl', 'rb')\n",
    "sentences = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "sentences['text']=sentences.apply(clean_string, axis=1)\n",
    "clean_PT_df = sentences\n",
    "\n",
    "pkl_file = open('../data/sentences_HI.pkl', 'rb')\n",
    "sentences = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "print(sentences.head())\n",
    "sentences['text']=sentences.apply(clean_string, axis=1)\n",
    "print(sentences.head())\n",
    "clean_HI_df = sentences\n",
    "\n",
    "#TODO: trying to remove blank values is not working\n",
    "#these methods are not working\n",
    "#clean_HI_df = clean_HI_df[\"text\"].apply(lambda x: pd.Series(x[0]) if x else pd.Series()).dropna()\n",
    "#clean_HI_df = clean_HI_df[clean_HI_df[\"text\"] != \"\"]\n",
    "#indexNames = clean_HI_df[len(clean_HI_df['text'])==0].index\n",
    "\n",
    "# Get names of indexes for which text is blank, currently all hindi unicode\n",
    "# by nature of how the data are loaded the hindi unicode is loaded third out of 4\n",
    "# This is fragile, but works for now\n",
    "indexNames = clean_HI_df[ clean_HI_df['sentence_id'] % 4 == 3 ].index\n",
    "\n",
    "# Delete these row indexes from dataFrame\n",
    "clean_HI_df.drop(indexNames , inplace=True)\n",
    "print(clean_HI_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            text  sentence_id\n",
      "0  [i, have, lost, my, passport]            1\n",
      "1    [someone, stole, my, money]            5\n",
      "2                         [help]            9\n",
      "3      [may, i, have, the, bill]           13\n",
      "4      [i, would, like, dessert]           17\n",
      "                              text  sentence_id\n",
      "0     [eu, perdi, meu, passaporte]            2\n",
      "1  [alguem, roubou, meu, dinheiro]            6\n",
      "2                        [socorro]           10\n",
      "3         [pode, trazer, a, conta]           14\n",
      "4    [eu, gostaria, de, sobremesa]           18\n",
      "                                         text  sentence_id\n",
      "1  [mainne, apana, paasaport, kho, diya, hai]            4\n",
      "3       [kisee, ne, mera, paisa, chura, liya]            8\n",
      "5                                     [madad]           12\n",
      "7         [kya, mujhe, bil, mil, sakata, hai]           16\n",
      "9                 [main, mithaee, chaahoonga]           20\n"
     ]
    }
   ],
   "source": [
    "#view the first 5 rows of one of the new dataframes to \n",
    "print(clean_EN_df.head())\n",
    "print(clean_PT_df.head())\n",
    "print(clean_HI_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming of words\n",
    "#Stemming refers to the process of reducing each word to its root or base.\n",
    "#For example “fishing,” “fished,” “fisher” all reduce to the stem “fish.”\n",
    "def stem_string(row):\n",
    "    row['text'] = [porter.stem(word) for word in row['text']]\n",
    "    return row['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            text  sentence_id\n",
      "0  [i, have, lost, my, passport]            1\n",
      "1     [someon, stole, my, money]            5\n",
      "2                         [help]            9\n",
      "3      [may, i, have, the, bill]           13\n",
      "4      [i, would, like, dessert]           17\n",
      "                              text  sentence_id\n",
      "0     [eu, perdi, meu, passaporte]            2\n",
      "1  [alguem, roubou, meu, dinheiro]            6\n",
      "2                        [socorro]           10\n",
      "3         [pode, trazer, a, conta]           14\n",
      "4    [eu, gostaria, de, sobremesa]           18\n",
      "                                         text  sentence_id\n",
      "1  [mainne, apana, paasaport, kho, diya, hai]            4\n",
      "3       [kisee, ne, mera, paisa, chura, liya]            8\n",
      "5                                     [madad]           12\n",
      "7         [kya, mujhe, bil, mil, sakata, hai]           16\n",
      "9                 [main, mithaee, chaahoonga]           20\n"
     ]
    }
   ],
   "source": [
    "#cioying to preserve integrity\n",
    "stemmed_clean_EN_df = clean_EN_df.copy()\n",
    "stemmed_clean_PT_df = clean_PT_df.copy()\n",
    "stemmed_clean_HI_df = clean_HI_df.copy()\n",
    "\n",
    "#Stem wirds to get to roots of words\n",
    "porter = PorterStemmer()\n",
    "stemmed_clean_EN_df['text'] = stemmed_clean_EN_df.apply(stem_string, axis=1)\n",
    "#stemmed_clean_PT_df['text'] = stemmed_clean_PT_df.apply(stem_string, axis=1)\n",
    "#stemmed_clean_HI_df['text'] = stemmed_clean_HI_df.apply(stem_string, axis=1)\n",
    "\n",
    "print(stemmed_clean_EN_df.head())\n",
    "print(stemmed_clean_PT_df.head())\n",
    "print(stemmed_clean_HI_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max sentence length\n",
    "def max_length(lines):\n",
    "    max_line_length = 0\n",
    "    for line in lines:\n",
    "        if(len(line) > max_line_length):\n",
    "            max_line_length=len(line)\n",
    "    return max_line_length\n",
    "\n",
    "\n",
    "#def max_length(lines):\n",
    "    #return max(len(line.split()) for line in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    # integer encode sequences\n",
    "    #X = tokenizer.texts_to_sequences(lines)\n",
    "    # pad sequences with 0 values\n",
    "    X = pad_sequences(X, maxlen=length, padding='post')\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras_preprocessing.text.Tokenizer object at 0x0000023CC2545208>\n",
      "306\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(stemmed_clean_EN_df['text'])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(stemmed_clean_EN_df['text'])\n",
    "\n",
    "print(eng_tokenizer)\n",
    "print(eng_vocab_size)\n",
    "print(eng_length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-46ad5781744a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrainX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencode_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meng_tokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meng_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstemmed_clean_EN_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sentence_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-54-20ba15e5d488>\u001b[0m in \u001b[0;36mencode_sequences\u001b[1;34m(tokenizer, length, lines)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mencode_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m# integer encode sequences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;31m# pad sequences with 0 values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'post'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras_preprocessing\\text.py\u001b[0m in \u001b[0;36mtexts_to_sequences\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    277\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0msequences\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m         \"\"\"\n\u001b[1;32m--> 279\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts_to_sequences_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    280\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtexts_to_sequences_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras_preprocessing\\text.py\u001b[0m in \u001b[0;36mtexts_to_sequences_generator\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    308\u001b[0m                                             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m                                             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 310\u001b[1;33m                                             self.split)\n\u001b[0m\u001b[0;32m    311\u001b[0m             \u001b[0mvect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras_preprocessing\\text.py\u001b[0m in \u001b[0;36mtext_to_word_sequence\u001b[1;34m(text, filters, lower, split)\u001b[0m\n\u001b[0;32m     41\u001b[0m     \"\"\"\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "\n",
    "#trainX = encode_sequences(eng_tokenizer, eng_length, stemmed_clean_EN_df['sentence_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "print('English Max Length: %d' % (eng_length))\n",
    "# prepare german tokenizer\n",
    "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
    "ger_length = max_length(dataset[:, 1])\n",
    "print('German Vocabulary Size: %d' % ger_vocab_size)\n",
    "print('German Max Length: %d' % (ger_length))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
