{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "import pickle\n",
    "from pickle import dump\n",
    "import re\n",
    "import string\n",
    "from unicodedata import normalize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas.core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(row):\n",
    "    # prepare regex for char filtering\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    # prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    # normalize unicode characters\n",
    "    row['text'] = normalize('NFD', row['text']).encode('ascii', 'ignore')\n",
    "    row['text'] = row['text'].decode('UTF-8')\n",
    "    # tokenize on white space\n",
    "    row['text'] = row['text'].split()\n",
    "    # convert to lowercase\n",
    "    row['text'] = [word.lower() for word in row['text']]\n",
    "    # remove punctuation from each token\n",
    "    row['text'] = [word.translate(table) for word in row['text']]\n",
    "    # remove non-printable chars form each token\n",
    "    row['text'] = [re_print.sub('', w) for w in row['text']]\n",
    "    # remove tokens with numbers in them\n",
    "    row['text'] = [word for word in row['text'] if word.isalpha()]\n",
    "    row['text'] = [x.strip(' ') for x in row['text']]\n",
    "    return row['text']\n",
    "\n",
    "\n",
    "#row['text'].strip().lower().replace('(', '').replace(')', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  text  sentence_id\n",
      "0       मैंने अपना पासपोर्ट खो दिया है            3\n",
      "1  mainne apana paasaport kho diya hai            4\n",
      "2          किसी ने मेरा पैसा चुरा लिया            7\n",
      "3       kisee ne mera paisa chura liya            8\n",
      "4                                  मदद           11\n",
      "                                         text  sentence_id\n",
      "0                                          []            3\n",
      "1  [mainne, apana, paasaport, kho, diya, hai]            4\n",
      "2                                          []            7\n",
      "3       [kisee, ne, mera, paisa, chura, liya]            8\n",
      "4                                          []           11\n",
      "                                         text  sentence_id\n",
      "1  [mainne, apana, paasaport, kho, diya, hai]            4\n",
      "3       [kisee, ne, mera, paisa, chura, liya]            8\n",
      "5                                     [madad]           12\n",
      "7         [kya, mujhe, bil, mil, sakata, hai]           16\n",
      "9                 [main, mithaee, chaahoonga]           20\n"
     ]
    }
   ],
   "source": [
    "# import and clean all language pkl files\n",
    "#EN = English, PT = Portuguese, HI = Hindi\n",
    "pkl_file = open('../data/sentences_EN.pkl', 'rb')\n",
    "sentences = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "sentences['text']=sentences.apply(clean_string, axis=1)\n",
    "clean_EN_df = sentences\n",
    "\n",
    "pkl_file = open('../data/sentences_PT.pkl', 'rb')\n",
    "sentences = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "sentences['text']=sentences.apply(clean_string, axis=1)\n",
    "clean_PT_df = sentences\n",
    "\n",
    "pkl_file = open('../data/sentences_HI.pkl', 'rb')\n",
    "sentences = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "print(sentences.head())\n",
    "sentences['text']=sentences.apply(clean_string, axis=1)\n",
    "print(sentences.head())\n",
    "clean_HI_df = sentences\n",
    "\n",
    "#TODO: trying to remove blank values is not working\n",
    "#these methods are not working\n",
    "#clean_HI_df = clean_HI_df[\"text\"].apply(lambda x: pd.Series(x[0]) if x else pd.Series()).dropna()\n",
    "#clean_HI_df = clean_HI_df[clean_HI_df[\"text\"] != \"\"]\n",
    "#indexNames = clean_HI_df[len(clean_HI_df['text'])==0].index\n",
    "\n",
    "# Get names of indexes for which text is blank, currently all hindi unicode\n",
    "# by nature of how the data are loaded the hindi unicode is loaded third out of 4\n",
    "# This is fragile, but works for now\n",
    "indexNames = clean_HI_df[ clean_HI_df['sentence_id'] % 4 == 3 ].index\n",
    "\n",
    "# Delete these row indexes from dataFrame\n",
    "clean_HI_df.drop(indexNames , inplace=True)\n",
    "print(clean_HI_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            text  sentence_id\n",
      "0  [i, have, lost, my, passport]            1\n",
      "1    [someone, stole, my, money]            5\n",
      "2                         [help]            9\n",
      "3      [may, i, have, the, bill]           13\n",
      "4      [i, would, like, dessert]           17\n",
      "                              text  sentence_id\n",
      "0     [eu, perdi, meu, passaporte]            2\n",
      "1  [alguem, roubou, meu, dinheiro]            6\n",
      "2                        [socorro]           10\n",
      "3         [pode, trazer, a, conta]           14\n",
      "4    [eu, gostaria, de, sobremesa]           18\n",
      "                                         text  sentence_id\n",
      "1  [mainne, apana, paasaport, kho, diya, hai]            4\n",
      "3       [kisee, ne, mera, paisa, chura, liya]            8\n",
      "5                                     [madad]           12\n",
      "7         [kya, mujhe, bil, mil, sakata, hai]           16\n",
      "9                 [main, mithaee, chaahoonga]           20\n"
     ]
    }
   ],
   "source": [
    "#view the first 5 rows of one of the new dataframes to \n",
    "print(clean_EN_df.head())\n",
    "print(clean_PT_df.head())\n",
    "print(clean_HI_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming of words\n",
    "#Stemming refers to the process of reducing each word to its root or base.\n",
    "#For example “fishing,” “fished,” “fisher” all reduce to the stem “fish.”\n",
    "def stem_string(row):\n",
    "    row['text'] = [porter.stem(word) for word in row['text']]\n",
    "    return row['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            text  sentence_id\n",
      "0  [i, have, lost, my, passport]            1\n",
      "1     [someon, stole, my, money]            5\n",
      "2                         [help]            9\n",
      "3      [may, i, have, the, bill]           13\n",
      "4      [i, would, like, dessert]           17\n",
      "                              text  sentence_id\n",
      "0     [eu, perdi, meu, passaporte]            2\n",
      "1  [alguem, roubou, meu, dinheiro]            6\n",
      "2                        [socorro]           10\n",
      "3         [pode, trazer, a, conta]           14\n",
      "4    [eu, gostaria, de, sobremesa]           18\n",
      "                                         text  sentence_id\n",
      "1  [mainne, apana, paasaport, kho, diya, hai]            4\n",
      "3       [kisee, ne, mera, paisa, chura, liya]            8\n",
      "5                                     [madad]           12\n",
      "7         [kya, mujhe, bil, mil, sakata, hai]           16\n",
      "9                 [main, mithaee, chaahoonga]           20\n"
     ]
    }
   ],
   "source": [
    "#cioying to preserve integrity\n",
    "stemmed_clean_EN_df = clean_EN_df.copy()\n",
    "stemmed_clean_PT_df = clean_PT_df.copy()\n",
    "stemmed_clean_HI_df = clean_HI_df.copy()\n",
    "\n",
    "#Stem wirds to get to roots of words\n",
    "porter = PorterStemmer()\n",
    "stemmed_clean_EN_df['text'] = stemmed_clean_EN_df.apply(stem_string, axis=1)\n",
    "#stemmed_clean_PT_df['text'] = stemmed_clean_PT_df.apply(stem_string, axis=1)\n",
    "#stemmed_clean_HI_df['text'] = stemmed_clean_HI_df.apply(stem_string, axis=1)\n",
    "\n",
    "print(stemmed_clean_EN_df.head())\n",
    "print(stemmed_clean_PT_df.head())\n",
    "print(stemmed_clean_HI_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max sentence length\n",
    "def max_length(lines):\n",
    "    max_line_length = 0\n",
    "    for line in lines:\n",
    "        if(len(line) > max_line_length):\n",
    "            max_line_length=len(line)\n",
    "    return max_line_length\n",
    "\n",
    "\n",
    "#def max_length(lines):\n",
    "    #return max(len(line.split()) for line in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    # integer encode sequences\n",
    "    #X = tokenizer.texts_to_sequences(lines)\n",
    "    # pad sequences with 0 values\n",
    "    X = pad_sequences(X, maxlen=length, padding='post')\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define NMT model\n",
    "def define_model(src_vocab, src_timesteps, n_units):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=1, activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "\n",
    "# define NMT model - original\n",
    "#def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "#    model = Sequential()\n",
    "#    model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "#    model.add(LSTM(n_units))\n",
    "#    model.add(RepeatVector(tar_timesteps))\n",
    "#    model.add(LSTM(n_units, return_sequences=True))\n",
    "#    model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "#    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras_preprocessing.text.Tokenizer object at 0x000001AC76744A58>\n",
      "306\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(stemmed_clean_EN_df['text'])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(stemmed_clean_EN_df['text'])\n",
    "\n",
    "print(eng_tokenizer)\n",
    "print(eng_vocab_size)\n",
    "print(eng_length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_EN=eng_tokenizer\n",
    "Y_EN=stemmed_clean_EN_df['sentence_id'].to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keras_preprocessing.text.Tokenizer"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_EN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define English model\n",
    "model = define_model(eng_vocab_size, eng_length, 256)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tokenizer' object has no attribute 'ndim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-bfac5aa3430a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_EN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_EN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 751\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    752\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'DataFrame'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstandardize_single_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'DataFrame'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstandardize_single_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_single_array\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     25\u001b[0m                 'Got tensor with shape: %s' % str(shape))\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[1;32melif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tokenizer' object has no attribute 'ndim'"
     ]
    }
   ],
   "source": [
    "model.fit(X_EN, Y_EN, epochs=5, batch_size=10,  verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# test on some training sequences\n",
    "print('train')\n",
    "evaluate_model(model, eng_tokenizer, trainX, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# test on some test sequences\n",
    "print('test')\n",
    "evaluate_model(model, eng_tokenizer, testX, test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
