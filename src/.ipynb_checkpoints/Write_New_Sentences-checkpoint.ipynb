{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "import pickle\n",
    "import sqlalchemy as sql\n",
    "from sqlalchemy import create_engine\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(row):\n",
    "    return re.sub('[!@#$%;:.?()\"\\'^\\{\\}\\[\\]|\\\\\\/<>=`~*&]', '', row['text']).strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set input values\n",
    "from_language=\"EN\"\n",
    "new_from_text = \"I love to code\"\n",
    "to_language=\"PT\"\n",
    "new_to_text = \"Eu amo codificar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The purpose of this code block is to find the test if our sentences are in the database\n",
    "\n",
    "#sanitize sentences\n",
    "new_from_text = re.sub('[!@#$%;:.?()\"\\'^\\{\\}\\[\\]|\\\\\\/<>=`~*&]', '', new_from_text).strip().lower()\n",
    "new_to_text = re.sub('[!@#$%;:.?()\"\\'^\\{\\}\\[\\]|\\\\\\/<>=`~*&]', '', new_to_text).strip().lower()\n",
    "\n",
    "\n",
    "# ====== Connection ====== #\n",
    "# Connecting to mysql by providing a sqlachemy engine\n",
    "engine = create_engine('mysql+pymysql://root:MBBmasters!@35.232.80.174/masters', echo=False)\n",
    "\n",
    "#Check for new from_text and insert if it is new\n",
    "df = pd.read_sql(\"select sentence_id from Sentences where language_key \\\n",
    "='\"+from_language+\"' and text = '\"+ new_from_text +\"'\", engine)\n",
    "\n",
    "if len(df.index) == 0:\n",
    "    #Insert due to no entries\n",
    "    data = [[new_from_text, from_language]] \n",
    "    new_sentence_insert = pd.DataFrame(data, columns = ['text', 'language_key']) \n",
    "    new_sentence_insert.to_sql('Sentences', con=engine, if_exists='append', index = False)\n",
    "    #Requery to get the value\n",
    "    df = pd.read_sql(\"select sentence_id from Sentences where language_key \\\n",
    "    ='\"+from_language+\"' and text = '\"+ new_from_text +\"'\", engine)\n",
    "\n",
    "from_sentence_id = df['sentence_id'].values[0]\n",
    "\n",
    "#Check for new to_text and insert if it is new\n",
    "df = pd.read_sql(\"select sentence_id from Sentences where language_key \\\n",
    "='\"+to_language+\"' and text = '\"+ new_to_text +\"'\", engine)\n",
    "\n",
    "if len(df.index) == 0:\n",
    "    #Insert due to no entries\n",
    "    data = [[new_to_text, to_language]] \n",
    "    new_sentence_insert = pd.DataFrame(data, columns = ['text', 'language_key']) \n",
    "    new_sentence_insert.to_sql('Sentences', con=engine, if_exists='append', index = False)\n",
    "    #Requery to get the value\n",
    "    df = pd.read_sql(\"select sentence_id from Sentences where language_key \\\n",
    "    ='\"+to_language+\"' and text = '\"+ new_to_text +\"'\", engine)\n",
    "\n",
    "to_sentence_id = df['sentence_id'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1604\n",
      "1605\n",
      "   translation_id\n",
      "0            4802\n",
      "   translation_id\n",
      "0            4803\n"
     ]
    }
   ],
   "source": [
    "#The goal of this text block is to insert the bidirectional translation if it is a new string\n",
    "\n",
    "df = pd.read_sql(\"select translation_id from Translations where \\\n",
    "sentence_id_1 =\"+str(from_sentence_id)+\" and sentence_id_2 = \"+ str(to_sentence_id), engine)\n",
    "\n",
    "if len(df.index) == 0:\n",
    "    data = [[from_sentence_id, to_sentence_id]] \n",
    "    new_translation_insert = pd.DataFrame(data, columns = ['sentence_id_1', 'sentence_id_2']) \n",
    "    new_translation_insert.to_sql('Translations', con=engine, if_exists='append', index = False)\n",
    "    df = pd.read_sql(\"select translation_id from Translations where \\\n",
    "    sentence_id_1 =\"+str(from_sentence_id)+\" and sentence_id_2 = \"+ str(to_sentence_id), engine)\n",
    "\n",
    "df = pd.read_sql(\"select translation_id from Translations where \\\n",
    "sentence_id_1 =\"+str(from_sentence_id)+\" and sentence_id_2 = \"+ str(to_sentence_id), engine)\n",
    "\n",
    "#print(df)\n",
    "\n",
    "df = pd.read_sql(\"select translation_id from Translations where \\\n",
    "sentence_id_2 =\"+str(from_sentence_id)+\" and sentence_id_1 = \"+ str(to_sentence_id), engine)\n",
    "\n",
    "if len(df.index) == 0:\n",
    "    data = [[from_sentence_id, to_sentence_id]] \n",
    "    new_translation_insert = pd.DataFrame(data, columns = ['sentence_id_2', 'sentence_id_1']) \n",
    "    new_translation_insert.to_sql('Translations', con=engine, if_exists='append', index = False)\n",
    "    df = pd.read_sql(\"select translation_id from Translations where \\\n",
    "    sentence_id_2 =\"+str(from_sentence_id)+\" and sentence_id_1 = \"+ str(to_sentence_id), engine)\n",
    "\n",
    "df = pd.read_sql(\"select translation_id from Translations where \\\n",
    "sentence_id_2 =\"+str(from_sentence_id)+\" and sentence_id_1 = \"+ str(to_sentence_id), engine)\n",
    "\n",
    "#print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      text  sentence_id\n",
      "0  i have lost my passport          801\n",
      "1   someone stole my money          805\n",
      "2                     help          809\n",
      "3      may i have the bill          813\n",
      "4     i would like dessert          817\n",
      "                        text  sentence_id\n",
      "0    eu perdi meu passaporte          802\n",
      "1  algum roubou meu dinheiro          806\n",
      "2                    socorro          810\n",
      "3        pode trazer a conta          814\n",
      "4   eu gostaria de sobremesa          818\n",
      "                                  text  sentence_id\n",
      "0                                               803\n",
      "1  mainne apana paasaport kho diya hai          804\n",
      "2                                               807\n",
      "3       kisee ne mera paisa chura liya          808\n",
      "4                                               811\n"
     ]
    }
   ],
   "source": [
    "# ====== Connection ====== #\n",
    "# Connecting to mysql by providing a sqlachemy engine\n",
    "engine = create_engine('mysql+pymysql://root:MBBmasters!@35.232.80.174/masters', echo=False)\n",
    "\n",
    "#Export English sentences in a pkl file (EN=English)\n",
    "df = pd.read_sql(\"select text, sentence_id from Sentences where language_key='EN'\", engine)\n",
    "#clean the strings, making them lowercase and removing some special characters\n",
    "df['text']=df.apply(clean_string, axis=1)\n",
    "output = open('../data/sentences_EN.pkl', 'wb')\n",
    "print(df.head())\n",
    "pickle.dump(df, output)\n",
    "output.close()\n",
    "\n",
    "#Export Portuguese sentences in a pkl file (PT=Portuguese)\n",
    "df = pd.read_sql(\"select text, sentence_id from Sentences where language_key='PT'\", engine)\n",
    "#clean the strings, making them lowercase and removing some special characters\n",
    "df['text']=df.apply(clean_string, axis=1)\n",
    "output = open('../data/sentences_PT.pkl', 'wb')\n",
    "print(df.head())\n",
    "pickle.dump(df, output)\n",
    "output.close()\n",
    "\n",
    "#Export Hindi sentences in a pkl file (HI=Hindi)\n",
    "df = pd.read_sql(\"select text, sentence_id from Sentences where language_key='HI'\", engine)\n",
    "#clean the strings, making them lowercase and removing some special characters\n",
    "df['text']=df.apply(clean_string, axis=1)\n",
    "output = open('../data/sentences_HI.pkl', 'wb')\n",
    "print(df.head())\n",
    "pickle.dump(df, output)\n",
    "output.close()\n",
    "\n",
    "#Export Translations sentences in a pkl file \n",
    "df = pd.read_sql(\"SELECT a.sentence_id as input_sentence_id, \\\n",
    "a.language_key as input_language_key, a.text as input_text, \\\n",
    "b.text as output_text, b.sentence_id as output_sentence_id, \\\n",
    "b.language_key as output_language_key FROM masters.Translations \\\n",
    "left join masters.Sentences a on a.sentence_id=Translations.sentence_id_1 \\\n",
    "left join masters.Sentences b on b.sentence_id=Translations.sentence_id_2\", engine)\n",
    "output = open('../data/translations.pkl', 'wb')\n",
    "pickle.dump(df, output)\n",
    "output.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   input_sentence_id input_language_key               input_text  \\\n",
      "0                801                 EN  I have lost my passport   \n",
      "1                801                 EN  I have lost my passport   \n",
      "2                801                 EN  I have lost my passport   \n",
      "3                802                 PT  Eu perdi meu passaporte   \n",
      "4                802                 PT  Eu perdi meu passaporte   \n",
      "\n",
      "                           output_text  output_sentence_id output_language_key  \n",
      "0              Eu perdi meu passaporte                 802                  PT  \n",
      "1       मैंने अपना पासपोर्ट खो दिया है                 803                  HI  \n",
      "2  mainne apana paasaport kho diya hai                 804                  HI  \n",
      "3       मैंने अपना पासपोर्ट खो दिया है                 803                  HI  \n",
      "4  mainne apana paasaport kho diya hai                 804                  HI  \n"
     ]
    }
   ],
   "source": [
    "# read python df back from the file\n",
    "pkl_file = open('../data/translations.pkl', 'rb')\n",
    "mydf = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "\n",
    "print(mydf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
