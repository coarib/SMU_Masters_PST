{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import itertools\n",
    "import datetime\n",
    "import pickle\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, LSTM, Lambda\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load from pkl file\n",
    "pkl_file = open('../data/sentences_EN.pkl', 'rb')\n",
    "test = pickle.load(pkl_file)\n",
    "test['EN'] =test['text']\n",
    "test =test.drop(columns=\"sentence_id\")\n",
    "test =test.drop(columns=\"text\")\n",
    "pkl_file.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#load from csv\n",
    "test = pd.read_csv(\"EN_PT.csv\",usecols=['EN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i have lost my passport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>someone stole my money</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>may i have the bill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i would like dessert</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        EN\n",
       "0  i have lost my passport\n",
       "1   someone stole my money\n",
       "2                     help\n",
       "3      may i have the bill\n",
       "4     i would like dessert"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.assign(Sentence = \"we are amazing friends\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EN</th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i have lost my passport</td>\n",
       "      <td>we are amazing friends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>someone stole my money</td>\n",
       "      <td>we are amazing friends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>help</td>\n",
       "      <td>we are amazing friends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>may i have the bill</td>\n",
       "      <td>we are amazing friends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i would like dessert</td>\n",
       "      <td>we are amazing friends</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        EN                Sentence\n",
       "0  i have lost my passport  we are amazing friends\n",
       "1   someone stole my money  we are amazing friends\n",
       "2                     help  we are amazing friends\n",
       "3      may i have the bill  we are amazing friends\n",
       "4     i would like dessert  we are amazing friends"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = 'https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\G7-DS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "    \n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "def text_to_word_list(text):\n",
    "    ''' Pre process and convert texts to a list of words '''\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "    text = text.split()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Prepare embedding\n",
    "def make_w2v_embeddings(df, embedding_dim=300, empty_w2v=False):\n",
    "    vocabulary = dict()\n",
    "    inverse_vocabulary = ['<unk>']  # '<unk>' will never be used, it is only a placeholder for the [0, 0, ....0] embedding\n",
    "    word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
    "\n",
    "    questions_cols = ['EN', 'Sentence']\n",
    "\n",
    "    # Iterate over the questions only of both training and test datasets\n",
    "    for dataset in [df]:\n",
    "        for index, row in dataset.iterrows():\n",
    "\n",
    "            # Iterate through the text of both questions of the row\n",
    "            for question in questions_cols:\n",
    "\n",
    "                sent = []  # q2n -> question numbers representation\n",
    "                for word in text_to_word_list(row[question]):\n",
    "\n",
    "                    # Check for unwanted words\n",
    "                    if word in stops and word not in word2vec.vocab:\n",
    "                        continue\n",
    "\n",
    "                    if word not in vocabulary:\n",
    "                        vocabulary[word] = len(inverse_vocabulary)\n",
    "                        sent.append(len(inverse_vocabulary))\n",
    "                        inverse_vocabulary.append(word)\n",
    "                    else:\n",
    "                        sent.append(vocabulary[word])\n",
    "\n",
    "                # Replace questions as word to question as number representation\n",
    "                dataset.set_value(index, question, sent)\n",
    "\n",
    "    embedding_dim = 300\n",
    "    embeddings = 1 * np.random.randn(len(vocabulary) + 1, embedding_dim)  # This will be the embedding matrix\n",
    "    embeddings[0] = 0  # So that the padding will be ignored\n",
    "\n",
    "    # Build the embedding matrix\n",
    "    for word, index in vocabulary.items():\n",
    "        if word in word2vec.vocab:\n",
    "            embeddings[index] = word2vec.word_vec(word)\n",
    "\n",
    "    #del word2vec\n",
    "    #word2vec.save_word2vec_format('EMBEDDING_FILE', binary=False)\n",
    "    return df, embeddings, word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_zero_padding(df, max_seq_length):\n",
    "    # Split to dicts\n",
    "    X = {'left': df['EN'], 'right': df['Sentence']}\n",
    "\n",
    "    # Zero padding\n",
    "    for dataset, side in itertools.product([X], ['left', 'right']):\n",
    "        dataset[side] = pad_sequences(dataset[side], padding='pre', truncating='post', maxlen=max_seq_length)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make word2vec embeddings\n",
    "embedding_dim = 300\n",
    "max_seq_length = 212\n",
    "\n",
    "test_set = test.copy()\n",
    "\n",
    "test_df, embeddings, word2vec = make_w2v_embeddings(test, embedding_dim=embedding_dim, empty_w2v=False)\n",
    "\n",
    "# Split to dicts and append zero padding.\n",
    "X_test = split_and_zero_padding(test_df, max_seq_length)\n",
    "\n",
    "# Make sure everything is ok\n",
    "assert X_test['left'].shape == X_test['right'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponent_neg_manhattan_distance(left, right):\n",
    "    ''' Helper function for the similarity estimate of the LSTMs outputs'''\n",
    "    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('malstm_trained_en.h5', custom_objects={'exponent_neg_manhattan_distance': exponent_neg_manhattan_distance})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict([X_test['left'], X_test['right']])\n",
    "idx = np.argmax(prediction)\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75217724"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1864343 ]\n",
      " [0.2077268 ]\n",
      " [0.00139167]\n",
      " [0.04001673]\n",
      " [0.08228357]\n",
      " [0.11394868]\n",
      " [0.12047512]\n",
      " [0.07582159]\n",
      " [0.13091348]\n",
      " [0.30556336]\n",
      " [0.24975017]\n",
      " [0.3383194 ]\n",
      " [0.20405579]\n",
      " [0.1952428 ]\n",
      " [0.23222817]\n",
      " [0.12037219]\n",
      " [0.25923547]\n",
      " [0.3408778 ]\n",
      " [0.30510157]\n",
      " [0.2725864 ]\n",
      " [0.12760966]\n",
      " [0.31613198]\n",
      " [0.24046808]\n",
      " [0.14550053]\n",
      " [0.00440373]\n",
      " [0.10351473]\n",
      " [0.05086734]\n",
      " [0.13020666]\n",
      " [0.03930987]\n",
      " [0.04517139]\n",
      " [0.09285522]\n",
      " [0.08344521]\n",
      " [0.08962798]\n",
      " [0.11826895]\n",
      " [0.1304662 ]\n",
      " [0.21397199]\n",
      " [0.2758755 ]\n",
      " [0.18127593]\n",
      " [0.26997033]\n",
      " [0.35972968]\n",
      " [0.06972925]\n",
      " [0.11322389]\n",
      " [0.17982802]\n",
      " [0.3184677 ]\n",
      " [0.16930312]\n",
      " [0.11393431]\n",
      " [0.17648284]\n",
      " [0.13870525]\n",
      " [0.1133661 ]\n",
      " [0.06097095]\n",
      " [0.14934707]\n",
      " [0.18219396]\n",
      " [0.07008293]\n",
      " [0.11330903]\n",
      " [0.30582345]\n",
      " [0.30763772]\n",
      " [0.04288143]\n",
      " [0.14798963]\n",
      " [0.01644535]\n",
      " [0.4223644 ]\n",
      " [0.36348945]\n",
      " [0.14560373]\n",
      " [0.13931662]\n",
      " [0.17293496]\n",
      " [0.14039883]\n",
      " [0.14268649]\n",
      " [0.1814806 ]\n",
      " [0.19006458]\n",
      " [0.04325335]\n",
      " [0.09624407]\n",
      " [0.30141297]\n",
      " [0.15594456]\n",
      " [0.22576253]\n",
      " [0.19340283]\n",
      " [0.432795  ]\n",
      " [0.1910964 ]\n",
      " [0.20380512]\n",
      " [0.2336539 ]\n",
      " [0.07659841]\n",
      " [0.0516891 ]\n",
      " [0.08481026]\n",
      " [0.21846968]\n",
      " [0.19453315]\n",
      " [0.31390214]\n",
      " [0.20050852]\n",
      " [0.2902747 ]\n",
      " [0.23249623]\n",
      " [0.18792826]\n",
      " [0.11339895]\n",
      " [0.17922464]\n",
      " [0.15283492]\n",
      " [0.08263491]\n",
      " [0.18780892]\n",
      " [0.20901354]\n",
      " [0.1520107 ]\n",
      " [0.18450557]\n",
      " [0.2163224 ]\n",
      " [0.30353555]\n",
      " [0.07355309]\n",
      " [0.3165769 ]\n",
      " [0.22626907]\n",
      " [0.0681773 ]\n",
      " [0.19671811]\n",
      " [0.3975265 ]\n",
      " [0.19246848]\n",
      " [0.06287754]\n",
      " [0.08755811]\n",
      " [0.13969043]\n",
      " [0.14020908]\n",
      " [0.16646312]\n",
      " [0.172724  ]\n",
      " [0.21121338]\n",
      " [0.17728156]\n",
      " [0.11762673]\n",
      " [0.07966645]\n",
      " [0.27197358]\n",
      " [0.34693566]\n",
      " [0.09606859]\n",
      " [0.19894621]\n",
      " [0.3714559 ]\n",
      " [0.21031138]\n",
      " [0.356011  ]\n",
      " [0.0524671 ]\n",
      " [0.08779576]\n",
      " [0.1114366 ]\n",
      " [0.31455678]\n",
      " [0.23098159]\n",
      " [0.33769852]\n",
      " [0.21922448]\n",
      " [0.0296238 ]\n",
      " [0.35375905]\n",
      " [0.08844657]\n",
      " [0.15520644]\n",
      " [0.29290363]\n",
      " [0.07558966]\n",
      " [0.16020288]\n",
      " [0.25117522]\n",
      " [0.00160848]\n",
      " [0.16235477]\n",
      " [0.2824404 ]\n",
      " [0.09495171]\n",
      " [0.23400025]\n",
      " [0.302245  ]\n",
      " [0.2568376 ]\n",
      " [0.24065755]\n",
      " [0.31625208]\n",
      " [0.28433427]\n",
      " [0.1890736 ]\n",
      " [0.0917147 ]\n",
      " [0.2796437 ]\n",
      " [0.1875749 ]\n",
      " [0.17452472]\n",
      " [0.30066037]\n",
      " [0.13667704]\n",
      " [0.24620765]\n",
      " [0.181394  ]\n",
      " [0.2720114 ]\n",
      " [0.1371061 ]\n",
      " [0.10642328]\n",
      " [0.15617187]\n",
      " [0.20020299]\n",
      " [0.19009446]\n",
      " [0.1595232 ]\n",
      " [0.35095853]\n",
      " [0.17037624]\n",
      " [0.07256399]\n",
      " [0.35091215]\n",
      " [0.15311508]\n",
      " [0.2690632 ]\n",
      " [0.21756636]\n",
      " [0.22532545]\n",
      " [0.22819015]\n",
      " [0.32552505]\n",
      " [0.2536482 ]\n",
      " [0.07815034]\n",
      " [0.08348455]\n",
      " [0.21017657]\n",
      " [0.2921787 ]\n",
      " [0.41376418]\n",
      " [0.75217724]\n",
      " [0.1215058 ]\n",
      " [0.06975352]\n",
      " [0.3796004 ]\n",
      " [0.0587853 ]\n",
      " [0.05521752]\n",
      " [0.31422454]\n",
      " [0.15132065]\n",
      " [0.12882315]\n",
      " [0.23105504]\n",
      " [0.3765775 ]\n",
      " [0.08476443]\n",
      " [0.19918367]\n",
      " [0.14870298]\n",
      " [0.24722297]\n",
      " [0.01652001]\n",
      " [0.28285474]\n",
      " [0.2607692 ]\n",
      " [0.09840048]\n",
      " [0.28027445]\n",
      " [0.22195777]\n",
      " [0.15492415]\n",
      " [0.13579972]\n",
      " [0.18976665]\n",
      " [0.41602603]\n",
      " [0.19918272]\n",
      " [0.0701492 ]\n",
      " [0.11184875]\n",
      " [0.2313136 ]\n",
      " [0.14935286]\n",
      " [0.07633394]\n",
      " [0.05761149]\n",
      " [0.26974487]\n",
      " [0.35196757]\n",
      " [0.04039585]\n",
      " [0.05640112]\n",
      " [0.05262893]]\n"
     ]
    }
   ],
   "source": [
    "#prediction=prediction.sort()\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EN          we are good friends\n",
       "Sentence    we are best friends\n",
       "Name: 179, dtype: object"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.iloc[idx]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
