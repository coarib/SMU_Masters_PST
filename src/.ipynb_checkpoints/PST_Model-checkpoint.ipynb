{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy.core._multiarray_umath'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-67cc59f9b8d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0meuclidean_distances\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mporter\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_multiarray_umath\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy.core._multiarray_umath'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "import pickle\n",
    "from pickle import dump\n",
    "import re\n",
    "import string\n",
    "from unicodedata import normalize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import numpy.core._multiarray_umath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy.core._multiarray_umath'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy.core._multiarray_umath'"
     ]
    },
    {
     "ename": "SystemError",
     "evalue": "<class '_frozen_importlib._ModuleLockManager'> returned a result with an error set",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;31mImportError\u001b[0m: numpy.core.multiarray failed to import",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;31mSystemError\u001b[0m: <class '_frozen_importlib._ModuleLockManager'> returned a result with an error set"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core._multiarray_umath failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;31mImportError\u001b[0m: numpy.core._multiarray_umath failed to import"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.umath failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;31mImportError\u001b[0m: numpy.core.umath failed to import"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.Proprocessing.Text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(row):\n",
    "    # prepare regex for char filtering\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    # prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    # normalize unicode characters\n",
    "    row['text'] = normalize('NFD', row['text']).encode('ascii', 'ignore')\n",
    "    row['text'] = row['text'].decode('UTF-8')\n",
    "    # tokenize on white space\n",
    "    row['text'] = row['text'].split()\n",
    "    # convert to lowercase\n",
    "    row['text'] = [word.lower() for word in row['text']]\n",
    "    # remove punctuation from each token\n",
    "    row['text'] = [word.translate(table) for word in row['text']]\n",
    "    # remove non-printable chars form each token\n",
    "    row['text'] = [re_print.sub('', w) for w in row['text']]\n",
    "    # remove tokens with numbers in them\n",
    "    row['text'] = [word for word in row['text'] if word.isalpha()]\n",
    "    row['text'] = [x.strip(' ') for x in row['text']]\n",
    "    return row['text']\n",
    "\n",
    "\n",
    "#row['text'].strip().lower().replace('(', '').replace(')', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and clean all language pkl files\n",
    "#EN = English, PT = Portuguese, HI = Hindi\n",
    "pkl_file = open('../data/sentences_EN.pkl', 'rb')\n",
    "sentences = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "sentences['text']=sentences.apply(clean_string, axis=1)\n",
    "clean_EN_df = sentences\n",
    "\n",
    "pkl_file = open('../data/sentences_PT.pkl', 'rb')\n",
    "sentences = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "sentences['text']=sentences.apply(clean_string, axis=1)\n",
    "clean_PT_df = sentences\n",
    "\n",
    "pkl_file = open('../data/sentences_HI.pkl', 'rb')\n",
    "sentences = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "sentences['text']=sentences.apply(clean_string, axis=1)\n",
    "clean_HI_df = sentences\n",
    "\n",
    "#TODO: trying to remove blank values is not working\n",
    "#these methods are not working\n",
    "#clean_HI_df = clean_HI_df[\"text\"].apply(lambda x: pd.Series(x[0]) if x else pd.Series()).dropna()\n",
    "#clean_HI_df = clean_HI_df[clean_HI_df[\"text\"] != \"\"]\n",
    "#indexNames = clean_HI_df[len(clean_HI_df['text'])==0].index\n",
    "\n",
    "# Get names of indexes for which text is blank, currently all hindi unicode\n",
    "# by nature of how the data are loaded the hindi unicode is loaded third out of 4\n",
    "# This is fragile, but works for now\n",
    "indexNames = clean_HI_df[ clean_HI_df['sentence_id'] % 4 == 3 ].index\n",
    "\n",
    "# Delete these row indexes from dataFrame\n",
    "clean_HI_df.drop(indexNames , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            text  sentence_id\n",
      "0  [i, have, lost, my, passport]            1\n",
      "1    [someone, stole, my, money]            5\n",
      "2                         [help]            9\n",
      "3      [may, i, have, the, bill]           13\n",
      "4      [i, would, like, dessert]           17\n",
      "                              text  sentence_id\n",
      "0     [eu, perdi, meu, passaporte]            2\n",
      "1  [alguem, roubou, meu, dinheiro]            6\n",
      "2                        [socorro]           10\n",
      "3         [pode, trazer, a, conta]           14\n",
      "4    [eu, gostaria, de, sobremesa]           18\n",
      "                                         text  sentence_id\n",
      "1  [mainne, apana, paasaport, kho, diya, hai]            4\n",
      "3       [kisee, ne, mera, paisa, chura, liya]            8\n",
      "5                                     [madad]           12\n",
      "7         [kya, mujhe, bil, mil, sakata, hai]           16\n",
      "9                 [main, mithaee, chaahoonga]           20\n"
     ]
    }
   ],
   "source": [
    "#view the first 5 rows of one of the new dataframes to \n",
    "print(clean_EN_df.head())\n",
    "print(clean_PT_df.head())\n",
    "print(clean_HI_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming of words\n",
    "#Stemming refers to the process of reducing each word to its root or base.\n",
    "#For example “fishing,” “fished,” “fisher” all reduce to the stem “fish.”\n",
    "def stem_string(row):\n",
    "    row['text'] = [porter.stem(word) for word in row['text']]\n",
    "    return row['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            text  sentence_id\n",
      "0  [i, have, lost, my, passport]            1\n",
      "1     [someon, stole, my, money]            5\n",
      "2                         [help]            9\n",
      "3      [may, i, have, the, bill]           13\n",
      "4      [i, would, like, dessert]           17\n",
      "                              text  sentence_id\n",
      "0      [eu, perdi, meu, passaport]            2\n",
      "1  [alguem, roubou, meu, dinheiro]            6\n",
      "2                        [socorro]           10\n",
      "3         [pode, trazer, a, conta]           14\n",
      "4    [eu, gostaria, de, sobremesa]           18\n",
      "                                        text  sentence_id\n",
      "1  [mainn, apana, paasaport, kho, diya, hai]            4\n",
      "3       [kise, ne, mera, paisa, chura, liya]            8\n",
      "5                                    [madad]           12\n",
      "7         [kya, mujh, bil, mil, sakata, hai]           16\n",
      "9                 [main, mithae, chaahoonga]           20\n"
     ]
    }
   ],
   "source": [
    "#cioying to preserve integrity\n",
    "stemmed_clean_EN_df = clean_EN_df.copy()\n",
    "stemmed_clean_PT_df = clean_PT_df.copy()\n",
    "stemmed_clean_HI_df = clean_HI_df.copy()\n",
    "\n",
    "#Stem wirds to get to roots of words\n",
    "porter = PorterStemmer()\n",
    "stemmed_clean_EN_df['text'] = stemmed_clean_EN_df.apply(stem_string, axis=1)\n",
    "stemmed_clean_PT_df['text'] = stemmed_clean_PT_df.apply(stem_string, axis=1)\n",
    "stemmed_clean_HI_df['text'] = stemmed_clean_HI_df.apply(stem_string, axis=1)\n",
    "\n",
    "print(stemmed_clean_EN_df.head())\n",
    "print(stemmed_clean_PT_df.head())\n",
    "print(stemmed_clean_HI_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max sentence length\n",
    "def max_length(lines):\n",
    "    return max(len(line.split()) for line in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-161-5554d17e2572>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# prepare english tokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0meng_tokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_tokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstemmed_clean_EN_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-147-a2695391dc54>\u001b[0m in \u001b[0;36mcreate_tokenizer\u001b[1;34m(lines)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# fit a tokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcreate_tokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(stemmed_clean_EN_df['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "print('English Max Length: %d' % (eng_length))\n",
    "# prepare german tokenizer\n",
    "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
    "ger_length = max_length(dataset[:, 1])\n",
    "print('German Vocabulary Size: %d' % ger_vocab_size)\n",
    "print('German Max Length: %d' % (ger_length))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
