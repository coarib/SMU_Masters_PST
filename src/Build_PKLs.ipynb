{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sqlalchemy as sql\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(row):\n",
    "    return row['text'].strip().lower().replace('(', '').replace(')', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      text  sentence_id\n",
      "0  i have lost my passport            1\n",
      "1   someone stole my money            5\n",
      "2                     help            9\n",
      "3      may i have the bill           13\n",
      "4     i would like dessert           17\n",
      "                         text  sentence_id\n",
      "0     eu perdi meu passaporte            2\n",
      "1  alguém roubou meu dinheiro            6\n",
      "2                     socorro           10\n",
      "3         pode trazer a conta           14\n",
      "4    eu gostaria de sobremesa           18\n",
      "                                  text  sentence_id\n",
      "0       मैंने अपना पासपोर्ट खो दिया है            3\n",
      "1  mainne apana paasaport kho diya hai            4\n",
      "2          किसी ने मेरा पैसा चुरा लिया            7\n",
      "3       kisee ne mera paisa chura liya            8\n",
      "4                                  मदद           11\n"
     ]
    }
   ],
   "source": [
    "# ====== Connection ====== #\n",
    "# Connecting to mysql by providing a sqlachemy engine\n",
    "engine = create_engine('mysql+mysqlconnector://root:smu@104.154.187.13/masters', echo=False)\n",
    "\n",
    "#Export English sentences in a pkl file (EN=English)\n",
    "df = pd.read_sql(\"select text, sentence_id from Sentences where language_key='EN'\", engine)\n",
    "#clean the strings, making them lowercase and removing some special characters\n",
    "df['text']=df.apply(clean_string, axis=1)\n",
    "output = open('../data/sentences_EN.pkl', 'wb')\n",
    "print(df.head())\n",
    "pickle.dump(df, output)\n",
    "output.close()\n",
    "\n",
    "#Export Portuguese sentences in a pkl file (PT=Portuguese)\n",
    "df = pd.read_sql(\"select text, sentence_id from Sentences where language_key='PT'\", engine)\n",
    "#clean the strings, making them lowercase and removing some special characters\n",
    "df['text']=df.apply(clean_string, axis=1)\n",
    "output = open('../data/sentences_PT.pkl', 'wb')\n",
    "print(df.head())\n",
    "pickle.dump(df, output)\n",
    "output.close()\n",
    "\n",
    "#Export Hindi sentences in a pkl file (HI=Hindi)\n",
    "df = pd.read_sql(\"select text, sentence_id from Sentences where language_key='HI'\", engine)\n",
    "#clean the strings, making them lowercase and removing some special characters\n",
    "df['text']=df.apply(clean_string, axis=1)\n",
    "output = open('../data/sentences_HI.pkl', 'wb')\n",
    "print(df.head())\n",
    "pickle.dump(df, output)\n",
    "output.close()\n",
    "\n",
    "#Export Translations sentences in a pkl file \n",
    "df = pd.read_sql(\"SELECT a.sentence_id as input_sentence_id, \\\n",
    "a.language_key as input_language_key, a.text as input_text, \\\n",
    "b.text as output_text, b.sentence_id as output_sentence_id, \\\n",
    "b.language_key as output_language_key FROM masters.Translations \\\n",
    "left join masters.Sentences a on a.sentence_id=Translations.sentence_id_1 \\\n",
    "left join masters.Sentences b on b.sentence_id=Translations.sentence_id_2\", engine)\n",
    "output = open('../data/translations.pkl', 'wb')\n",
    "pickle.dump(df, output)\n",
    "output.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   input_sentence_id input_language_key               input_text  \\\n",
      "0                  1                 EN  I have lost my passport   \n",
      "1                  1                 EN  I have lost my passport   \n",
      "2                  1                 EN  I have lost my passport   \n",
      "3                  2                 PT  Eu perdi meu passaporte   \n",
      "4                  2                 PT  Eu perdi meu passaporte   \n",
      "\n",
      "                           output_text  output_sentence_id output_language_key  \n",
      "0              Eu perdi meu passaporte                   2                  PT  \n",
      "1       मैंने अपना पासपोर्ट खो दिया है                   3                  HI  \n",
      "2  mainne apana paasaport kho diya hai                   4                  HI  \n",
      "3       मैंने अपना पासपोर्ट खो दिया है                   3                  HI  \n",
      "4  mainne apana paasaport kho diya hai                   4                  HI  \n"
     ]
    }
   ],
   "source": [
    "# read python df back from the file\n",
    "pkl_file = open('../data/translations.pkl', 'rb')\n",
    "mydf = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "\n",
    "print(mydf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
